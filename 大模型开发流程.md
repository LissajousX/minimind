# 大模型开发完整流程 - 基于MiniMind项目讲解

## 一、准备阶段

### 1. 确定模型规模与架构

首先需要确定模型的规模(参数量)和架构，这决定了模型的能力上限和资源需求。

**MiniMind中的实现：**
- MiniMind提供了多个规模的模型：
  - MiniMind2-Small: 26M参数
  - MiniMind2-MoE: 145M参数 
  - MiniMind2: 104M参数
- 采用Transformer解码器架构，在`model/LMConfig.py`中定义：
  ```python
  def __init__(
      self,
      dim: int = 512,       # 隐藏层维度
      n_layers: int = 8,    # 层数
      n_heads: int = 8,     # 注意力头数
      n_kv_heads: int = 2,  # KV注意力头数
      vocab_size: int = 6400,  # 词表大小
      ...
  ```

### 2. 训练分词器

分词器负责将文本转换为token序列，是模型处理文本的基础。

**MiniMind中的实现：**
- 使用`scripts/train_tokenizer.py`训练自定义分词器：
  ```python
  # 设置训练器并添加特殊token
  trainer = trainers.BpeTrainer(
      vocab_size=6400,
      special_tokens=["<unk>", "<s>", "</s>"],
      show_progress=True,
      initial_alphabet=pre_tokenizers.ByteLevel.alphabet()
  )
  ```
- 生成的分词器文件位于`model/minimind_tokenizer/`

## 二、模型开发阶段

### 3. 实现模型架构

根据设计实现模型的核心架构。

**MiniMind中的实现：**
- 在`model/model.py`中实现了完整的模型架构：
  - RMSNorm：归一化层
  - Attention：多头注意力机制
  - FeedForward：前馈神经网络
  - MiniMindBlock：Transformer基本块
  - MiniMindLM：整体语言模型

### 4. 准备数据集

收集并预处理训练数据。

**MiniMind中的实现：**
- 在`model/dataset.py`中定义了多种数据集类：
  - PretrainDataset：预训练数据集
  - SFTDataset：监督微调数据集
  - DPODataset：直接偏好优化数据集
- README中提到的数据文件：
  ```
  ./dataset/
  ├── pretrain_hq.jsonl (1.6GB)
  ├── sft_mini_512.jsonl (1.2GB)
  └── ...
  ```

## 三、训练阶段

### 5. 预训练

让模型学习语言的基本规律和知识。

**MiniMind中的实现：**
- 使用`train_pretrain.py`进行预训练：
  ```python
  python train_pretrain.py
  ```
- 预训练数据：`pretrain_hq.jsonl`
- 训练过程关键代码：
  ```python
  loss = loss_fct(
      res.logits.view(-1, res.logits.size(-1)),
      Y.view(-1)
  ).view(Y.size())
  ```

### 6. 监督微调(SFT)

教模型如何按照人类期望的方式回答问题。

**MiniMind中的实现：**
- 使用`train_full_sft.py`进行监督微调：
  ```python
  python train_full_sft.py
  ```
- SFT数据：`sft_mini_512.jsonl`
- 数据格式：
  ```json
  {
    "conversations": [
      {"role": "user", "content": "你好"},
      {"role": "assistant", "content": "你好！"}
    ]
  }
  ```

### 7. LoRA参数高效微调

使用低秩适配技术，以极低的计算资源对模型进行特定任务的微调。

**MiniMind中的实现：**
- 使用`train_lora.py`进行LoRA微调：
  ```python
  python train_lora.py --lora_name lora_医疗 --data_path ./dataset/medical_data.jsonl
  ```
- 核心实现在`model/model_lora.py`中：
  ```python
  class LoRA(nn.Module):
      def __init__(self, in_features, out_features, rank):
          self.A = nn.Linear(in_features, rank, bias=False)  # 低秩矩阵A
          self.B = nn.Linear(rank, out_features, bias=False)  # 低秩矩阵B
  ```
- 相比全量微调的优势：
  - 训练参数量仅为原模型的约1%
  - 训练成本从1.3元降至约0.3元
  - 训练时间从1小时减少到约15分钟
  - 可为不同任务保存多个轻量级LoRA模块

### 8. 强化学习(RLHF/DPO)

通过人类反馈进一步优化模型表现。

**MiniMind中的实现：**
- 使用`train_dpo.py`进行直接偏好优化：
  ```python
  python train_dpo.py
  ```
- DPO数据：`dpo.jsonl`
- 数据格式包含preferred和rejected两种回答

### 9. 知识蒸馏(可选)

从大模型向小模型迁移知识。

**MiniMind中的实现：**
- 使用`train_distillation.py`进行知识蒸馏
- 使用`train_distill_reason.py`专门针对推理能力的蒸馏

## 四、评估与应用阶段

### 10. 模型评估

评估模型在各种任务上的表现。

**MiniMind中的实现：**
- 使用`eval_model.py`评估模型：
  ```python
  python eval_model.py --model_mode 1  # 测试SFT模型
  ```
- README中包含了在多个基准测试上的结果，如C-Eval、CMMLU等

### 11. 模型部署与应用

将训练好的模型部署为应用。

**MiniMind中的实现：**
- 网页演示：`scripts/web_demo.py`
  ```python
  streamlit run web_demo.py
  ```
- API服务：`scripts/serve_openai_api.py`

## MiniMind项目的特点

1. **极致轻量化**：最小模型仅25.8M，普通电脑可训练
2. **全流程开源**：从预训练到推理全过程代码开源
3. **原生实现**：所有核心算法使用PyTorch原生重构
4. **模块化设计**：可以灵活选择训练阶段和模型规模

## 实际训练成本

README中指出，最小版本（MiniMind2-Small）的训练成本极低：
- 预训练：约1.1小时，约1.43元
- SFT训练：约1小时，约1.3元
- 总计：约2.1小时，约2.73元

这也是MiniMind项目的核心价值：让大模型开发变得平易近人，任何人都能以极低成本入门大模型开发。
